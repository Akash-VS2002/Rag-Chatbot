# ğŸ“š RAG Chatbot with LangGraph Memory (Flask + Groq + Chroma)

A Retrieval-Augmented Generation (RAG) chatbot that can answer questions from **PDF documents** and **YouTube videos**, built using **LangChain**, **LangGraph memory**, **Chroma vector database**, and **Groq LLM** with a **Flask web interface**.

This project demonstrates how to combine document retrieval with conversational memory to create an intelligent assistant that remembers previous context during a session.

---

# ğŸš€ Features

âœ… PDF document ingestion
âœ… YouTube audio transcription (Whisper)
âœ… Vector database with ChromaDB
âœ… HuggingFace embeddings
âœ… Groq LLM (fast inference)
âœ… LangGraph conversation memory
âœ… Flask web UI
âœ… Source document & page reference display
âœ… Modular architecture (config + ingest + app)

---

# ğŸ§  How It Works (Architecture)

1. **Ingestion Phase**

   * Load PDFs and YouTube audio
   * Convert speech â†’ text
   * Split into chunks
   * Generate embeddings
   * Store in Chroma vector database

2. **Chat Phase**

   * User asks a question
   * Relevant chunks retrieved from vector DB
   * Context + question sent to LLM
   * LangGraph maintains conversation memory
   * Response returned with references

---

# ğŸ“‚ Project Structure

```
RAG-CHATBOT/
â”‚â”€â”€ app.py                  # Flask chatbot app
â”‚â”€â”€ ingest.py               # Document ingestion pipeline
â”‚â”€â”€ config.py               # Configuration settings
â”‚â”€â”€ .env                    # Environment variables template
â”‚â”€â”€ README.md
â”‚
â”œâ”€â”€ data/                   # PDF documents
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ youtube/            # Downloaded YouTube audio
â”‚   â””â”€â”€ chroma/             # Vector database
â”‚
â””â”€â”€ templates/
    â””â”€â”€ index.html          # Flask UI
```

---

# âš™ï¸ Installation

## 1ï¸âƒ£ Clone Repository

```bash
git clone https://github.com/yourusername/rag-chatbot.git
cd rag-chatbot
```

---

## 2ï¸âƒ£ Create Virtual Environment

```bash
python -m venv venv
source venv/bin/activate   # Mac/Linux
venv\Scripts\activate      # Windows
```

---

## 3ï¸âƒ£ Install Dependencies

If using pip:

```bash
pip install -r requirements.txt
```

Or using uv:

```bash
uv sync
```

---

# ğŸ” Environment Variables

Create `.env` file in project root:

```
GROQ_API_KEY=your_groq_api_key_here
```

You can copy from:

```
.env.example
```

---

# ğŸ“¥ System Requirements

âš ï¸ Required for YouTube transcription:

Install **FFmpeg**

Windows:
https://ffmpeg.org/download.html

After installation â†’ add to PATH.

---

# ğŸ“Š Step 1: Document Ingestion

Run once to create vector database.

```bash
python ingest.py
```

This will:

* Process PDFs from `data/`
* Download & transcribe YouTube video
* Create embeddings
* Store in `docs/chroma/`

---

# ğŸ’¬ Step 2: Run Chatbot

```bash
python app.py
```

Open browser:

```
http://127.0.0.1:5000
```

---

# ğŸ§¾ Configuration

Edit `config.py`:

```python
YOUTUBE_VIDEO_URL = "your_url"
PDF_SOURCE_DIRECTORY = "data"
CHROMA_PERSIST_DIRECTORY = "docs/chroma"

EMBEDDING_MODEL_NAME = "intfloat/multilingual-e5-large"
CHUNK_SIZE = 2028
CHUNK_OVERLAP = 250
```

---

# ğŸ§  Memory System (LangGraph)

The chatbot uses:

```
MemorySaver()
```

This enables:

âœ… Conversation history
âœ… Context continuity
âœ… Session-based memory

Each session uses a `thread_id` to maintain state.

---

# ğŸ” Retrieval Process

1. Similarity search (Top K = 3)
2. Context extraction
3. Prompt construction
4. LLM response generation
5. Source references returned

---

# ğŸ¤– Model Used

LLM: **Llama 3.1 8B Instant (Groq)**
Embeddings: **multilingual-e5-large**
Speech-to-Text: **Faster-Whisper**

---

# ğŸ–¥ï¸ API Endpoint

POST `/chat`

Request:

```json
{
  "message": "What is machine learning?"
}
```

Response:

```json
{
  "response": "Answer text..."
}
```

---

# ğŸ“Œ Example Use Cases

* Academic question answering
* Research assistant
* Lecture video Q&A
* Knowledge base chatbot
* Internal company documentation bot

---

# âš¡ Performance Tips (Low RAM PCs)

If using 4GB RAM laptop:

Change embedding model:

```python
all-MiniLM-L6-v2
```

Use smaller Whisper model:

```
small
```

---

# ğŸ› ï¸ Future Improvements

* User authentication
* Chat history database
* Docker deployment
* Streaming responses
* Multi-document upload UI
* Cloud vector database

---

# ğŸ Troubleshooting

### Chroma DB not loading

Make sure:

```
docs/chroma/
```

exists and ingestion completed.

---

### Whisper errors

Install FFmpeg properly.

---

### API key error

Check `.env` file.

---

# ğŸ™Œ Acknowledgements

* LangChain
* LangGraph
* Groq
* HuggingFace
* ChromaDB

---

â— Important Setup Instructions
1ï¸âƒ£ Add Your YouTube Video Link

Open config.py and update the YouTube URL:

YOUTUBE_VIDEO_URL: str = "PASTE_YOUR_YOUTUBE_LINK_HERE"



2ï¸âƒ£ Add PDF Files

Place all your PDF documents inside the data/ folder.

Project structure example:

data/
   book.pdf
   notes.pdf
   research_paper.pdf

The ingestion script will automatically read all PDFs from this folder.